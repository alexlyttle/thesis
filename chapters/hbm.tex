% chapters/hbm.tex
%
% Copyright 2022 Alexander Lyttle.
%
% This work may be distributed and/or modified under the conditions of the
% LaTeX Project Public License (LPPL) version 1.3 or later.
%
% The latest version of this license is in
% https://www.latex-project.org/lppl.txt and version 1.3 or later is part of
% all distributions of LaTeX version 2005/12/01 or later.
%
%
\chapter{Hierarchical Bayesian Models}

\todo{Introduce hierarchical models as a concept. Maybe like in Hall, start with context about generative models. Then show that when we expend models to, e.g. many stars, it is useful to extend our prior to represent correlations in the stellar population. In a sense, creating priors informed by population-level distributions. Here is a good opportunity to reference past work, and maybe expand on some of the background given in the introduction to \citep{Lyttle.Davies.ea2021}.}

\section[Stellar distances]{Stellar distances in an open cluster analogue}\label{sec:hbm-dist}

\newcommand{\appmag}{\ensuremath{\mathrm{v}}}
\newcommand{\absmag}{\ensuremath{\mathrm{V}}}

In this section, we use the example of measuring distances to stars in an open cluster to demonstrate a hierarchical Bayesian model. This example is loosely based on the work of \citet{Leistedt.Hogg2017}, which presents a hierarchical model of the colour-magnitude diagram to improve distances from \emph{Gaia} \citep{GaiaCollaboration.Prusti.ea2016}. 

We created a dataset analogous to an open cluster of \(N_\mathrm{stars}\) stars. We gave each \(i\)-th star a dimensionless distance (\(d_i\)) from the observer drawn randomly from a normal distribution with a mean of 10 and standard deviation of 0.1. Then, we converted these to parallax using the relation \(\varpi = d^{-1}\). We also gave each star an absolute visual magnitude (\(\absmag_i\)) drawn randomly from a standard normal distribution. To get the apparent magnitudes (\(\appmag_i\)) for each star, we used the relation \(\appmag_i = \absmag_i + 5 \log_{10} d_i\).

\begin{table}[tb]
    \centering
    \caption{Simulated dimensionless distance, magnitudes and parallax for \(N_\mathrm{stars}=20\) belonging to an open cluster analogue.}
    \label{tab:hbm-data}
    \input{tables/hbm-data}
\end{table}

We simulated noisy observations of \(\varpi_i\) and \(\appmag\) by randomly drawing from a normal distribution centred on their true values with standard deviations of \(\sigma_{\appmag,i} = 0.1\) and \(\sigma_{\varpi,i} = 0.01\) respectively. We repeated this for \(N_\mathrm{stars}=20\) stars and present the true values and observables in Table \ref{tab:hbm-data}.

\subsection{Simple model}

We started with a simple model which treats each star independently. Using Bayes' theorem, we write the posterior probability density of the model parameters, \(d_i, \absmag_i\) given the observed parameters \(\varpi_i, \appmag_i\) as,
%
\begin{equation}
    p(d_i, \absmag_i \mid \varpi_i, \appmag_i) \propto p(\varpi_i, \appmag_i \mid d_i, \absmag_i) \, p(d_i, \absmag_i).
\end{equation}
%
The likelihood is given by \(p(\varpi_i, \appmag_i \mid d_i, \absmag_i)\) and the prior probability of the parameters is \(p(d_i, \absmag_i)\).

We modelled the likelihood as the product of normal distributions, assuming \(\varpi_i\) and \(\appmag_i\) were observed independently,
%
\begin{equation}
    p(\varpi_i, \appmag_i \mid d_i, \absmag_i) = \mathcal{N}(\varpi_i \mid d_i^{\,-1}, \sigma_{\varpi, i}^2) \, \mathcal{N}(\appmag_i \mid \absmag_i + 5\log_{10}d_i \, , \sigma_{\appmag, i}^2),\label{eq:hbm-like}
\end{equation}
%
where \(\mathcal{N}(x \,|\, \mu, \sigma^2)\) is a normal distribution over \(x\) with a mean of \(\mu\) and variance of \(\sigma^2\).

We assumed stars in the cluster were equally likely to be between a distance of 0 and 20. We also assumed the absolute magnitudes were likely to be normally distributed centred on 0 and scaled by 10. Therefore, the prior probability of the model parameters is,
%
\begin{equation}
    p(d_i, \absmag_i) = \mathcal{U}(d_i \mid 0, 20) \, \mathcal{N}(\absmag_i \mid 0, 100),
\end{equation}
%
where \(\mathcal{U}(x \,|\, a, b)\) is a uniform distribution over \(x\) from \(a\) to \(b\). We chose weakly-informative priors on the parameters, given that we know the true values for this example. However, these priors are fairly unrealistic and should be adapted to represent our expectation. For example, the exponential prior from \citet{Bailer-Jones.Rybizki.ea2018} would me more appropriate when observing stars radially outward from the galactic disk.

\begin{figure}[tb]
    \centering
    \includegraphics{figures/simple-pgm.pdf}
    \caption{Probabilistic graphical model for the simple (non-hierarchical) model. Parameters are given by circular nodes and connected by arrows showing the direction of dependency. Observed parameters are shaded, and fixed parameters are given by filled dots. The box represents a set of parameters belonging to the \(i\)-th star.}
    \label{fig:simple-pgm}
\end{figure}

In Figure \ref{fig:simple-pgm}, we show a probabilistic graphical model of the simple model. This shows the connections between parameters in the model. There is no hierarchy in this model because each parameter exists within the box, meaning no information is shared between stars. However, we know that the stellar distances are correlated. For example, it is highly unlikely that one star in the cluster is at a distance of 5 and another is at a distance of 15. If we can exploit this expectation, we could improve our prior and thus improve the inference of \(d_i\) and \(\absmag_i\).

\subsection{Hierarchical model}

In this section, we present a hierarchical Bayesian model (HBM) which includes the known correlation between distances to the stars in this open cluster analogue. We assumed the stars are all members of the same open cluster. Therefore, their distances can be modelled by some distribution. For this example, we assumed that each distance is drawn from a normal distribution characterised by new \emph{hyperparameters} \(\mu_d\) and \(\sigma_d\),
%
\begin{equation}
    d_i \sim \mathcal{N}(\mu_d, \sigma_d^2).
\end{equation}
%
The hyperparameters are so-called because they take a single value for the population of stars. Hence, the hierarchy of the model arises as some parameters represent a population of stars, while others represent the individual stars in the population.

Each stellar distance, \(d_i\), is no longer independent. Hence, we modified the posterior probability distribution to account for this new correlation,
%
\begin{equation}
    p(\mu_d, \sigma_d, \vect{d}, \vect{\absmag} \mid \vect{\varpi}, \vect{\appmag}) \propto p(\vect{\varpi}, \vect{\appmag} \mid \vect{d}, \vect{\absmag}) \, p(\vect{d} \mid \mu_d, \sigma_d) \, p(\mu_d, \sigma_d, \vect{\absmag}),
\end{equation}
%
where \(p(\vect{\varpi}, \vect{\appmag} \mid \vect{d}, \vect{\absmag})\) is the likelihood --- a product over Equation \ref{eq:hbm-like},
%
\begin{equation}
    p(\vect{\varpi}, \vect{\appmag} \mid \vect{d}, \vect{\absmag}) = \prod_{i=1}^{N_\mathrm{stars}} p(\varpi_i, \appmag_i \mid d_i, \absmag_i).
\end{equation}
%
Our posterior now depends on all stars in the population, so we used a bold typeface to represent the set of individual stellar parameters. For example, \(\vect{d} \equiv d_1, \dots, d_{N_\mathrm{stars}}\).

We modelled \(d_i\) as a random variable which depends on the hyperparameters. This means our prior is now the product of a conditional distribution, \(p(\vect{d} \,|\, \mu_d, \sigma_d)\) and a prior on the remaining independent parameters, \(p(\mu_d, \sigma_d, \vect{\absmag})\). We write the conditional distribution for distance as a product of normal distributions,
%
\begin{equation}
    p(\vect{d} \mid \mu_d, \sigma_d) = \prod_{i=1}^{N_\mathrm{stars}} \mathcal{N}(d_i \mid \mu_d, \sigma_d^2),
\end{equation}
%
where the prior probability distribution for \(\mu_d\) and \(\sigma_d\) is,
%
\begin{equation}
    p(\mu_d, \sigma_d) = \mathcal{U}(\mu_d \mid 0, 20) \, \mathcal{N}(\ln\sigma_d \mid - \ln 10, 1).
\end{equation}
%
We chose to use a log-normal prior for \(\sigma_d\) to ensure that the parameter is positive. The standard deviation of the prior on \(\sigma_d\) approximately corresponds to a fractional uncertainty of 100 per cent. Finally, we inherited the prior on \(\absmag_i\) from the simple model.

\begin{figure}[tb]
    \centering
    \includegraphics{figures/hbm-pgm.pdf}
    \caption{Probabilistic graphical model extension of Figure \ref{fig:simple-pgm} but for the HBM. The hyperparameters exist outside of the box to show that they govern the population of stars.}
    \label{fig:hbm-pgm}
\end{figure}

We show the probabilistic graphical model of the HBM in Figure \ref{fig:hbm-pgm}. Here, we show how all individual stellar parameters depend on \(\mu_d\) and \(\sigma_d\). We can imagine extending this framework to multiple levels, or adding additionally hyperparameters.

\subsection{Inferring the model parameters}

To infer the model parameters, we need to calculate the marginalised posterior distributions for each parameter. We could obtain this analytically by integrating the full posterior distribution over all model parameters except for the parameter of interest. Alternatively, we can approximate the marginalised posterior using a Markov Chain Monte Carlo (MCMC) sampling algorithm. We chose the latter approach because it is scalable to more complicated models where the marginalisation is not analytically solvable.

We used the No U-Turn Sampler \citep[NUTS;][]{Hoffman.Gelman2014} as implemented in the \textsc{NumPyro} Python package \citep{Phan.Pradhan.ea2019,Bingham.Chen.ea2019} to sample from the approximate posterior distribution for both models. We took 1500 samples, with the first 500 samples discarded as `warmup' steps, for each of 10 chains. For the HBM, we increased the target accept probability from 0.8 to 0.98 to minimise the number of divergences encountered during sampling. The resulting marginalised posterior samples amounted to \num{10000} per parameter.

\subsection{Comparing the models}



\begin{figure}[p]
    \centering
    \includegraphics{figures/hbm-results.pdf}
    \caption{Plots comparing the simple model (\emph{blue}) to the HBM (\emph{orange}). \emph{Left column:} Split violin-plots showing the difference between marginalised posterior distributions for the parameters and their true values. \emph{Right column:} Kernel density estimates of the parameter-truth differences over all \(N_\mathrm{stars}\). Observed values of the parameters are given by empty black circles.}
    \label{fig:hbm-results}
\end{figure}


\begin{figure}[tb]
    \centering
    \includegraphics{figures/hbm-dist-corr.pdf}
    \caption{}
    \label{fig:hbm-corr}
\end{figure}

\subsection{Scaling with the number of stars}

We repeated the HBM for \(N_\mathrm{stars}=80\) and \(N_\mathrm{stars}=320\). We generated the true and observed parameters in the same way as described at the beginning of \ref{sec:hbm-dist}.

\section[Stellar inference]{Inferring stellar parameters}

Typically stars are modelling independently of each other. However, there a several factors which could correlate from star-to-star. \todo{Introduce the idea of hierarchically modelling stars. Finish by explaining that this is slow and difficult to do ad hoc, e.g. propose initial parameters from population distribution and evolve a model. We would like a way to emulate MESA etc. This leads to the next section on emulating stellar models with machine learning.}
